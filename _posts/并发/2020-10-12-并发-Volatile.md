---
title: Volatile
categories:
- 并发
tags: [Java并发]
---

* 目录
{:toc}

## volatile 总结

1. 保障 long/double 类型变量访问操作的原子性（32位平台低位和高位是分开读写的）
2. 保障可见性（多线程共享变量）
3. 保障有序性（Happens-before）
4. 性能介于普通变量和临界区中的变量之间
5. 保障对象的安全发布（双重检查锁定法的单例）
6. 间接保障复合操作的原子性与可见性（Immutable Object模式）
7. Scala使用 @volatile 注解
8. volatile 声明数组时并不保证其中元素的可见性，应使用AtomicIntegerArray替代

## volatile 的实现原理

在 x86 处理器下通过工具获取 JIT 编译器生成的汇编指令来看看对 volatile 进行写操作 CPU 会做什么事情。

Java 代码：
```java
volatile Singleton instance = new Singleton();
```

汇编代码：
```
0x01a3de1d: movb $0x0,0x1104800(%esi);

0x01a3de24: lock addl $0x0,(%esp);
```

有 volatile 变量修饰的共享变量进行写操作的时候会多第二行汇编代码，通过查 IA-32 架构软件开发者手册可知，lock 前缀的指令在多核处理器下会引发了两件事情。

* 将当前处理器缓存行的数据写回到系统内存。
* 这个写回内存的操作会引起在其他 CPU 里缓存了该内存地址的数据无效。
  
处理器为了提高处理速度，不直接和内存进行通讯，而是先将系统内存的数据读到内部缓存（L1，L2，L3等）后再进行操作，但操作完之后不知道何时会写到内存，如果对声明了 volatile 变量进行写操作，JVM 就会向处理器发送一条 Lock 前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题，所以在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器要对这个数据进行修改操作的时候，会强制重新从系统内存里把数据读到处理器缓存里。这两件事情在 IA-32 软件开发者架构手册的第三册的多处理器管理章节（第八章）中有详细阐述。

**Lock 前缀指令会引起处理器缓存回写到内存。** Lock 前缀指令导致在执行指令期间，声言处理器的 LOCK# 信号。在多处理器环境中，LOCK# 信号确保在声言该信号期间，处理器可以独占使用任何共享内存。（因为它会锁住总线，导致其他 CPU 不能访问总线，不能访问总线就意味着不能访问系统内存），但是在最近的处理器里，LOCK＃信号一般不锁总线，而是锁缓存，毕竟锁总线开销比较大。在 8.1.4 章节有详细说明锁定操作对处理器缓存的影响，对于 Intel486 和 Pentium 处理器，在锁操作时，总是在总线上声言 LOCK#信号。但在 P6 和最近的处理器中，如果访问的内存区域已经缓存在处理器内部，则不会声言 LOCK#信号。相反地，它会锁定这块内存区域的缓存并回写到内存，并使用缓存一致性机制来确保修改的原子性，此操作被称为“缓存锁定”，**缓存一致性机制会阻止同时修改被两个以上处理器缓存的内存区域数据。**

**一个处理器的缓存回写到内存会导致其他处理器的缓存无效。** IA-32 处理器和 Intel 64 处理器使用 MESI（修改，独占，共享，无效）控制协议去维护内部缓存和其他处理器缓存的一致性。在多核处理器系统中进行操作的时候，IA-32 和 Intel 64 处理器能嗅探其他处理器访问系统内存和它们的内部缓存。它们使用嗅探技术保证它的内部缓存，系统内存和其他处理器的缓存的数据在总线上保持一致。例如在 Pentium 和 P6 family 处理器中，如果通过嗅探一个处理器来检测其他处理器打算写内存地址，而这个地址当前处理共享状态，那么正在嗅探的处理器将无效它的缓存行，在下次访问相同内存地址时，强制执行缓存行填充。

## volatile 的使用优化

著名的 Java 并发编程大师 Doug lea 在 JDK7 的并发包里新增一个队列集合类 LinkedTransferQueue，他在使用 volatile 变量时，用一种追加字节的方式来优化队列出队和入队的性能。这种方式看起来很神奇，但如果深入理解处理器架构就能理解其中的奥秘。让我们先来看看 LinkedTransferQueue 这个类，它使用一个内部类类型来定义队列的头队列（Head）和尾节点（tail），而这个内部类 PaddedAtomicReference 相对于父类 AtomicReference 只做了一件事情，就将共享变量追加到 64 字节。我们可以来计算下，一个对象的引用占 4 个字节，它追加了 15 个变量共占 60 个字节，再加上父类的 Value 变量，一共 64 个字节。

数据填充是伪共享的常见的解决方案。（Java8 -XX:-RestrictContended + @Contended注解）
```java

/** head of the queue */
private transient final PaddedAtomicReference < QNode > head;

/** tail of the queue */

private transient final PaddedAtomicReference < QNode > tail;


static final class PaddedAtomicReference < T > extends AtomicReference < T > {

   // enough padding for 64bytes with 4byte refs 
   Object p0, p1, p2, p3, p4, p5, p6, p7, p8, p9, pa, pb, pc, pd, pe;

   PaddedAtomicReference(T r) {

       super(r);

   }

}

public class AtomicReference < V > implements java.io.Serializable {

   private volatile V value;

   // 省略其他代码 
｝
```

**为什么追加 64 字节能够提高并发编程的效率呢？** 因为对于英特尔酷睿 i7，酷睿， Atom 和 NetBurst， Core Solo 和 Pentium M 处理器的 L1，L2 或 L3 缓存的高速缓存行是 64 个字节宽，不支持部分填充缓存行，这意味着如果队列的头节点和尾节点都不足 64 字节的话，处理器会将它们都读到同一个高速缓存行中，在多处理器下每个处理器都会缓存同样的头尾节点，当一个处理器试图修改头接点时会将整个缓存行锁定，那么在缓存一致性机制的作用下，会导致其他处理器不能访问自己高速缓存中的尾节点，而队列的入队和出队操作是需要不停修改头接点和尾节点，所以在多处理器的情况下将会严重影响到队列的入队和出队效率。Doug lea 使用追加到 64 字节的方式来填满高速缓冲区的缓存行，避免头接点和尾节点加载到同一个缓存行，使得头尾节点在修改时不会互相锁定。

**是不是在使用 Volatile 变量时都应该追加到 64 字节呢？** 不是的。在两种场景下不应该使用这种方式。第一：**缓存行非 64 字节宽的处理器** ，如 P6 系列和奔腾处理器，它们的 L1 和 L2 高速缓存行是 32 个字节宽。第二：**共享变量不会被频繁的写** 。因为使用追加字节的方式需要处理器读取更多的字节到高速缓冲区，这本身就会带来一定的性能消耗，共享变量如果不被频繁写的话，锁的几率也非常小，就没必要通过追加字节的方式来避免相互锁定。

## 一些术语

**高速缓存（Cache）。** 处理器借以访问主内存（RAM）的小容量高速存取部件。高速缓存可理解为由硬件实现的散列表（Hash Table）。处理器并不是直接访问主内存，处理器对主内存的读、写操作都是通过高速缓存进行的。

**缓存条目（Cache Entry）。** 高速缓存内部存储单元。相当于散列表中的条目。

**缓存行（Cache Line）。** 高速缓存与主内存之间的数据交换（传输）的最小单元。由于共享变量在 CPU 缓存中的存储是以缓存行为单位，一个缓存行可以存储多个变量（存满当前缓存行的字节数）；而CPU对缓存的修改又是以缓存行为最小单位的，那么就会出现伪共享问题。缓存行可以简单的理解为 CPU Cache 中的最小缓存单位，今天的 CPU 不再是按字节访问内存，而是以 64 字节为单位的块（chunk）拿取，称为一个缓存行。当你读一个特定的内存地址，整个缓存行将从主存加载入缓存，并且访问同一个缓存行内的其它值的开销是很小的。

**CPU 的三级缓存。** 由于 CPU 的速度远远大于内存速度，所以 CPU 设计者们就给 CPU 加上了缓存（CPU Cache）。以免运算被内存速度拖累。（就像我们写代码把共享数据做Cache不想被DB存取速度拖累一样），CPU Cache 分成了三个级别：L1、L2、L3。越靠近 CPU 的缓存越快也越小。所以 L1 缓存很小但很快，并且紧靠着在使用它的 CPU 内核。L2 大一些，也慢一些，并且仍然只能被一个单独的 CPU 核使用。L3 在现代多核机器中更普遍，仍然更大，更慢，并且被单个插槽上的所有 CPU 核共享。最后，你拥有一块主存，由全部插槽上的所有 CPU 核共享。当 CPU 执行运算的时候，它先去 L1 查找所需的数据，再去 L2，然后是 L3，最后如果这些缓存中都没有，所需的数据就要去主内存拿。走得越远，运算耗费的时间就越长。所以如果你在做一些很频繁的事，你要确保数据在 L1 缓存中。

**缓存关联性。** 目前常用的缓存设计是 N 路组关联（N-Way Set Associative Cache），它的原理是把一个缓存按照 N 个 Cache Line 作为一组（Set），缓存按组划为等分。每个内存块能够被映射到相对应的 Set 中的任意一个缓存行中。比如一个 16 路缓存，16 个 Cache Line 作为一个 Set，每个内存块能够被映射到相对应的 Set 中的16 个 CacheLine 中的任意一个。一般地，具有一定相同低 bit 位地址的内存块将共享同一个 Set。

**缓存行填充（Cache Line Fill）。** 当处理器识别到从内存中读取操作数是可缓存的，处理器读取整个缓存行到适当的缓存（L1、L2、L3 的或所有）。

**伪共享（False Sharing）。** CPU 缓存系统中是以缓存行为单位存储的。目前主流的 CPU Cache 的缓存行大小都是 64 Bytes。在多线程情况下，如果需要修改“共享同一个缓存行的变量”，就会无意中影响彼此的性能，这就是伪共享。

**缓存命中（Cache Hit）。** 如果进行高速缓存行填充操作的内存位置仍然是下次处理器访问的地址时，处理器从缓存中读取操作数，而不是从内存。

**写命中（Write Hit）。** 当处理器将操作数写回到一个内存缓存的区域时，它首先会检查这个缓存的内存地址是否在缓存行中，如果存在一个有效的缓存行，则处理器将这个操作数写回到缓存，而不是写回到内存，这个操作被称为写命中。

**写缺失（Write Misses The Cache）。** 一个有效的缓存行被写入到不存在的内存区域。

**缓存一致性协议（Cache Coherence Protocol）。** 处理器用于确保通过高速缓存访问主内存达到与直接访问主内存等效（性能差异除外）的一套协议。

**无效化队列（Invalidate/Probe/Coherence Queue）。** 处理器用于暂存无效化消息（Invalidation Message）的存储部件。无效化消息的作用是一个处理器修改了某个共享变量之后借以通知其他处理器其对共享变量的更新，以便其他处理器能够将其高速缓存中的相应缓存行置为无效。

**写缓存器（Store/Write Buffer）。** 处理器内部用于暂存写入高速缓存（以写入主内存）数据的容量极小的存储部件。

**内存屏障（Memory Barrier/Fence）。** 对作用于内存读、写操作的一类特殊处理器指令的统称。

**程序顺序（Program Order）。** 目标代码中指定的一组内存操作的顺序。

**感知顺序（Perceived Order）。** 一个处理器对其他处理器上执行的一组内存操作所观察到的顺序。